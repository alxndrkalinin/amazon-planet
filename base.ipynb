{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.misc\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/user/data/amazon_planet'\n",
    "TRAIN_DIR = 'train-jpg'\n",
    "TRAIN_DATA = 'train_v2.csv'\n",
    "TEST_DIR = 'test-jpg'\n",
    "IMG_EXT = '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(DATA_DIR + '/' + TRAIN_DATA)\n",
    "labels_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "labels_list = list(chain.from_iterable([tags.split(\" \") for tags in labels_df['tags'].values]))\n",
    "labels_set = set(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = sorted(labels_set)\n",
    "labels_map = {l: i for i, l in enumerate(labels)}\n",
    "y_map = {v: k for k, v in labels_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_s = pd.Series(labels_list).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "angles = np.arange(0, 360, 45)\n",
    "offsets = np.arange(0, 12)\n",
    "img_resize = (224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_imgs(*args):\n",
    "    path,size_,mode = list(args[0])\n",
    "    bgr_img = cv2.imread(path)\n",
    "    b,g,r = cv2.split(bgr_img)\n",
    "    img = cv2.merge([r,g,b])\n",
    "    \n",
    "    # resize\n",
    "    if mode == 'val':\n",
    "        img = cv2.resize(img, img_resize, interpolation = cv2.INTER_AREA)\n",
    "    else:\n",
    "        x, y = np.random.choice(offsets, 2)\n",
    "        img = img[x:x+img_resize[0], y:y+img_resize[0]]\n",
    "    \n",
    "    # scale\n",
    "    img = img/ 255.0\n",
    "    \n",
    "    # augment\n",
    "    if mode == 'train':\n",
    "        for i in xrange(2):\n",
    "            if np.random.randint(2) == 1:\n",
    "                img = np.flip(img, i)\n",
    "        num_rows, num_cols = img.shape[:2]\n",
    "        rotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2), np.random.choice(angles), 1)\n",
    "        img = cv2.warpAffine(img, rotation_matrix, (num_cols, num_rows), borderMode=cv2.BORDER_REFLECT_101)\n",
    "        \n",
    "    return img\n",
    "\n",
    "def get_batch(files_path,img_resize,dir_path, mode):\n",
    "    x_train = []\n",
    "    with ThreadPoolExecutor(cpu_count()) as pool:\n",
    "        for img_array in pool.map(get_imgs,[(dir_path+file_path+'.jpg',img_resize,mode) for file_path in files_path]):\n",
    "                x_train.append(img_array)\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels_df = pd.read_csv(\"../input/train_v2.csv\")\n",
    "labels = sorted(set(chain.from_iterable([tags.split(\" \") for tags in labels_df['tags'].values])))\n",
    "labels_map = {l: i for i, l in enumerate(labels)}\n",
    "\n",
    "files_path = []\n",
    "y_label = []\n",
    "for file_name, tags in labels_df.values:\n",
    "    files_path.append(file_name)\n",
    "    targets = np.zeros(len(labels_map))\n",
    "    for t in tags.split(' '):\n",
    "        targets[labels_map[t]] = 1\n",
    "    y_label.append(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(files_path, y_label,test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Amazon(nn.Module):\n",
    "    def __init__(self, pretrained_model_1):\n",
    "        super(Amazon, self).__init__()\n",
    "        self.pretrained_model_1 = pretrained_model_1\n",
    "        self.pretrained_model_1.fc = nn.Linear(pretrained_model1.fc.in_features, 17)\n",
    "        self.pretrained_model_1 = nn.DataParallel(self.pretrained_model_1)\n",
    "#         self.classifier = \n",
    "        # self.pretrained_model_2 = pretrained_model_2\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc1 = nn.Linear(2000,1000)\n",
    "#         self.fc2 = nn.Linear(1000,len(labels_set)) # create layer\n",
    "#         self.sigmoid = torch.nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "#         model_1 = self.relu(self.pretrained_model_1(x))\n",
    "        #model_2 = self.relu(self.pretrained_model_2(x))\n",
    "        #out1 = torch.cat((model_1,model_2),1)\n",
    "        return self.pretrained_model_1(x)\n",
    "\n",
    "# #pretrained_model1 = models.densenet169(pretrained=True)\n",
    "# pretrained_model1 = models.resnet50(pretrained=True)#in fact, this should be set as true\n",
    "\n",
    "# model = Amazon(pretrained_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_path = DATA_DIR + '/' + TRAIN_DIR + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = './model_3.pkl'\n",
    "pretrained_model1 = models.resnet50(pretrained=True)\n",
    "for param in pretrained_model1.parameters():\n",
    "    param.requires_grad = False\n",
    "model = Amazon(pretrained_model1)\n",
    "# if path:\n",
    "#     model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pretrained_model1\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input_ = Variable(torch.from_numpy(np.transpose(get_batch(X_train[0:32],(224,224),dir_path,'val'), (0, 3,1, 2)))).float()\n",
    "# o = model(input_)\n",
    "# o.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exp_lr_scheduler(optimizer, epoch, init_lr=0.0001, lr_decay_epoch=30):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (0.1**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.pretrained_model_1.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_x,train_y,valid_x, valid_y,epoch,num_model,img_resize,dir_path,lr_scheduler,model,batch_size):\n",
    "#     print model\n",
    "    torch.cuda.set_device(0)\n",
    "    criterion = nn.BCELoss().cuda()\n",
    "    optimizer = torch.optim.SGD(model.pretrained_model_1.module.fc.parameters(), lr=1e-04, momentum=0.9)\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=1e-04, momentum=0.9)\n",
    "    model.cuda()\n",
    "    best_score = 0\n",
    "    for epo in range(epoch):\n",
    "        print 'epo: ' + str(epo)\n",
    "#         if epo>0:\n",
    "        optimizer = lr_scheduler(optimizer, epo)\n",
    "        num_shuffle = np.random.permutation(range(len(train_y)))\n",
    "        for step in range(len(train_x)/batch_size):\n",
    "            x_batch = np.transpose(get_batch(train_x[num_shuffle[step*batch_size:(step+1)*batch_size]],img_resize,dir_path,'train'), (0, 3,1, 2))\n",
    "            input_var = Variable(torch.from_numpy(x_batch)).float().cuda()\n",
    "            target_var = Variable(torch.from_numpy(train_y[num_shuffle[step*batch_size:(step+1)*batch_size]])).cuda().float()\n",
    "            output = model(input_var)\n",
    "            output.clamp(min=1e-8,max=1e+8)\n",
    "            loss = criterion(output, target_var)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if step % 30 ==0:\n",
    "                valid_pred = validate(model,valid_x, valid_y,32,img_resize,dir_path)\n",
    "                threshhold = [0.2]*17\n",
    "                score = fbeta_score(np.array(valid_y)[:len(valid_pred)], np.array(valid_pred) >threshhold, beta=2, average='samples')\n",
    "                print(\"epo: \"+str(epo)+\" step: \"+str(step)+\"  score: \"+str(score))\n",
    "                print('loss: '+ str(loss.data.cpu().numpy()[0].astype(float)))\n",
    "                path = './model_'+str(num_model)+'.pkl'\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    torch.save(model.state_dict(), path)\n",
    "                    print(\"save in : \"+ path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(model_,x_valid, y_valid,batch_val_size,img_resize,dir_path):\n",
    "    p_valid = []\n",
    "    pred_true = []\n",
    "    for i in range(len(x_valid)/batch_val_size-1):\n",
    "        #target = target.cuda(async=True)\n",
    "        x_batch = np.transpose(get_batch(x_valid[i*batch_val_size:(i+1)*batch_val_size],img_resize,dir_path,'val'), (0, 3,1, 2))\n",
    "        input_var = Variable(torch.from_numpy(x_batch)).float().cuda()\n",
    "        target_var = Variable(torch.from_numpy(y_valid[i*batch_val_size:(i+1)*batch_val_size])).cuda().float()\n",
    "        output = model_(input_var).data.cpu().numpy().astype(float)\n",
    "        p_valid.extend(output)\n",
    "        pred_true.extend(y_valid[i*batch_val_size:(i+1)*batch_val_size])\n",
    "    return p_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_pred(x_test,batch_test_size,path,dir_path):\n",
    "    pretrained_model1 = models.resnet18(pretrained=True)\n",
    "    model = Amazon(pretrained_model1)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "#     torch.cuda.set_device(2)\n",
    "    model.cuda()\n",
    "    p_test = []\n",
    "    for step in range(len(x_test)/batch_test_size):\n",
    "        if step%20==0:\n",
    "            print(step)\n",
    "        x_batch = np.transpose(get_batch(x_test[step*batch_test_size:(step+1)*batch_test_size],img_resize,dir_path,'test'), (0, 3,1, 2))\n",
    "        input_var = Variable(torch.from_numpy(x_batch)).float().cuda()\n",
    "        output = model(input_var).data.cpu().numpy().astype(float)\n",
    "        p_test.extend(output)\n",
    "    left_data = get_batch(x_test[-(len(x_test)- len(x_test)/batch_test_size*batch_test_size):],img_resize,dir_path,'test')\n",
    "    input_var = Variable(torch.from_numpy(np.transpose(left_data, (0, 3,1, 2)))).float().cuda()\n",
    "    output = model(input_var).data.cpu().numpy().astype(float)\n",
    "    p_test.extend(output)\n",
    "    return p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.pretrained_model_1.module.fc\n",
    "cv2.BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch=5\n",
    "num_model=4\n",
    "batch_size = 256\n",
    "train(np.array(X_train),np.array(Y_train),np.array(X_valid),np.array(Y_valid),epoch,num_model,img_resize,dir_path,\n",
    "     exp_lr_scheduler, model,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = './model_'+ str(num_model) + '.pkl'\n",
    "y_pred = test_pred(np.array(X_valid),batch_size,path,dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import fbeta_score\n",
    "def get_optimal_threshhold(true_label, prediction, iterations = 100):\n",
    "\n",
    "    best_threshhold = [0.2]*17    \n",
    "    for t in range(17):\n",
    "        best_fbeta = 0\n",
    "        temp_threshhold = [0.2]*17\n",
    "        for i in range(iterations):\n",
    "            temp_value = i / float(iterations)\n",
    "            temp_threshhold[t] = temp_value\n",
    "            temp_fbeta = fbeta(true_label, prediction >temp_threshhold)\n",
    "            if  temp_fbeta>best_fbeta:\n",
    "                best_fbeta = temp_fbeta\n",
    "                best_threshhold[t] = temp_value\n",
    "    return best_threshhold\n",
    "\n",
    "def fbeta(true_label, prediction):\n",
    "    return fbeta_score(true_label, prediction, beta=2, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_threshhold = get_optimal_threshhold(np.array(Y_valid)[:len(y_pred)], np.array(y_pred), iterations = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sub = pd.read_csv(DATA_DIR + '/sample_submission_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_sub = test_sub.image_name.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "files_name_test1 = sample_sub[:len(os.listdir(DATA_DIR + \"/test-jpg/\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = './model_0.pkl'\n",
    "predictions = test_pred(files_name_test1,batch_size,path,DATA_DIR + \"/test-jpg/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_filename = sample_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_predictions(predictions, labels_map, thresholds):\n",
    "    \"\"\"\n",
    "    Return the predictions mapped to their labels\n",
    "    :param predictions: the predictions from the predict() method\n",
    "    :param labels_map: the map\n",
    "    :param thresholds: The threshold of each class to be considered as existing or not existing\n",
    "    :return: the predictions list mapped to their labels\n",
    "    \"\"\"\n",
    "    predictions_labels = []\n",
    "    for prediction in predictions:\n",
    "        labels = [labels_map[i] for i, value in enumerate(prediction) if value > thresholds[i]]\n",
    "        predictions_labels.append(labels)\n",
    "\n",
    "    return predictions_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_labels = map_predictions(predictions, y_map, best_threshhold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags_list = [None] * len(predicted_labels)\n",
    "for i, tags in enumerate(predicted_labels):\n",
    "    tags_list[i] = ' '.join(map(str, tags))\n",
    "final_data = [[filename.split(\".\")[0], tags] for filename, tags in zip(x_test_filename, tags_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final_data, columns=['image_name', 'tags'])\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df.to_csv('./submission_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
