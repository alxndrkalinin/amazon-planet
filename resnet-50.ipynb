{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import hashlib\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "import six\n",
    "\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import (ImageDataGenerator, Iterator,\n",
    "                                       array_to_img, img_to_array, load_img)\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/user/data/amazon_planet'\n",
    "TRAIN_DIR = 'train-jpg'\n",
    "TRAIN_DATA = 'train_v2.csv'\n",
    "TEST_DIR = 'test-jpg'\n",
    "IMG_EXT = '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "input_channels = 3\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 96\n",
    "learning_rate = 0.001\n",
    "lr_decay = 1e-4\n",
    "\n",
    "valid_data_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "\n",
    "df_train = pd.read_csv(DATA_DIR + '/' + TRAIN_DATA)\n",
    "df_test = pd.read_csv(DATA_DIR + '/sample_submission_v2.csv')\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "labels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\n",
    "\n",
    "labels = ['blow_down',\n",
    " 'bare_ground',\n",
    " 'conventional_mine',\n",
    " 'blooming',\n",
    " 'cultivation',\n",
    " 'artisinal_mine',\n",
    " 'haze',\n",
    " 'primary',\n",
    " 'slash_burn',\n",
    " 'habitation',\n",
    " 'clear',\n",
    " 'road',\n",
    " 'selective_logging',\n",
    " 'partly_cloudy',\n",
    " 'agriculture',\n",
    " 'water',\n",
    " 'cloudy']\n",
    "\n",
    "label_map = {'agriculture': 14,\n",
    " 'artisinal_mine': 5,\n",
    " 'bare_ground': 1,\n",
    " 'blooming': 3,\n",
    " 'blow_down': 0,\n",
    " 'clear': 10,\n",
    " 'cloudy': 16,\n",
    " 'conventional_mine': 2,\n",
    " 'cultivation': 4,\n",
    " 'habitation': 9,\n",
    " 'haze': 6,\n",
    " 'partly_cloudy': 13,\n",
    " 'primary': 7,\n",
    " 'road': 11,\n",
    " 'selective_logging': 12,\n",
    " 'slash_burn': 8,\n",
    " 'water': 15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_arrays_from_file(path):\n",
    "    while 1:\n",
    "    f = open(path)\n",
    "    for line in f:\n",
    "        # create numpy arrays of input data\n",
    "        # and labels, from each line in the file\n",
    "        x1, x2, y = process_line(line)\n",
    "        yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40479/40479 [01:10<00:00, 575.61it/s]\n",
      "100%|██████████| 61191/61191 [01:45<00:00, 581.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40479, 224, 224, 3)\n",
      "(40479, 17)\n"
     ]
    }
   ],
   "source": [
    "for f, tags in tqdm(df_train.values, miniters=1000):\n",
    "    img = cv2.imread(DATA_DIR + '/' + TRAIN_DIR + '/{}.jpg'.format(f))\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1 \n",
    "    x_train.append(cv2.resize(img, (input_size, input_size)))\n",
    "    y_train.append(targets)\n",
    "\n",
    "for f, tags in tqdm(df_test.values, miniters=1000):\n",
    "    img = cv2.imread(DATA_DIR + '/' + TEST_DIR + '/{}.jpg'.format(f))\n",
    "    x_test.append(cv2.resize(img, (input_size, input_size)))\n",
    "    \n",
    "y_train = np.array(y_train, np.uint8)\n",
    "x_train = np.array(x_train, np.float32)/255.\n",
    "x_test  = np.array(x_test, np.float32)/255.\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "nfolds = 5\n",
    "\n",
    "num_fold = 0\n",
    "sum_score = 0\n",
    "\n",
    "yfull_test = []\n",
    "yfull_train =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = applications.resnet50.ResNet50(weights = \"imagenet\", include_top=False, input_shape = (input_size, input_size, 3))\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:]:\n",
    "    layer.trainable = False\n",
    "    print 'Not training: ' + str(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ResNet-50\n",
    "# add a global spatial average pooling layer\n",
    "x = model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(17, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_final = Model(inputs = model.input, outputs = predictions)\n",
    "# model_final.compile(loss = \"binary_crossentropy\", optimizer = 'adam', metrics=[\"accuracy\"])\n",
    "# model_final.compile(loss = \"binary_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(nfolds, shuffle=True, random_state=1)\n",
    "\n",
    "for train_index, test_index in kf.split(x_train):\n",
    "#     start_time_model_fitting = time.time()\n",
    "\n",
    "    X_train = x_train[train_index]\n",
    "    Y_train = y_train[train_index]\n",
    "    X_valid = x_train[test_index]\n",
    "    Y_valid = y_train[test_index]\n",
    "    \n",
    "    datagen.fit(X_train)\n",
    "\n",
    "    num_fold += 1\n",
    "    print('Start KFold number {} from {}'.format(num_fold, nfolds))\n",
    "    print('Split train: ', len(X_train), len(Y_train))\n",
    "    print('Split valid: ', len(X_valid), len(Y_valid))\n",
    "\n",
    "    kfold_weights_path = os.path.join('', 'weights_kfold_' + str(num_fold) + '.h5')\n",
    "\n",
    "    epochs_arr = [30, 10, 10]\n",
    "    learn_rates = [0.001, 0.0001, 0.00001]\n",
    "\n",
    "    for learn_rate, epochs in zip(learn_rates, epochs_arr):\n",
    "        opt  = optimizers.SGD(lr=learn_rate)\n",
    "        model_final.compile(loss='binary_crossentropy', # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n",
    "                      optimizer=opt,\n",
    "                      metrics=['accuracy'])\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "        ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "        TQDMNotebookCallback()]\n",
    "\n",
    "#         model_final.fit(x = X_train, y= Y_train, validation_data=(X_valid, Y_valid),\n",
    "#               batch_size=96, verbose=2, epochs=epochs, callbacks=callbacks, shuffle=True)\n",
    "        model_final.fit_generator(datagen.flow(X_train, Y_train, batch_size=96), validation_data=(X_valid, Y_valid),\n",
    "                                  steps_per_epoch=len(X_train) / 96,\n",
    "                                  verbose=2, epochs=epochs, callbacks=callbacks, workers=8, use_multiprocessing=False)\n",
    "    \n",
    "        for layer in model.layers[:]:\n",
    "            layer.trainable = True\n",
    "            print 'Not training: ' + str(layer)\n",
    "\n",
    "    if os.path.isfile(kfold_weights_path):\n",
    "        model_final.load_weights(kfold_weights_path)\n",
    "\n",
    "    p_valid = model_final.predict(X_valid, batch_size = 128, verbose=2)\n",
    "    print(fbeta_score(Y_valid, np.array(p_valid) > 0.2, beta=2, average='samples'))\n",
    "\n",
    "    p_train = model_final.predict(x_train, batch_size =128, verbose=2)\n",
    "    yfull_train.append(p_train)\n",
    "\n",
    "    p_test = model_final.predict(x_test, batch_size = 128, verbose=2)\n",
    "    yfull_test.append(p_test)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = np.array(yfull_test[0])\n",
    "for i in range(1, nfolds):\n",
    "    result += np.array(yfull_test[i])\n",
    "result /= nfolds\n",
    "result = pd.DataFrame(result, columns = labels)\n",
    "result\n",
    "\n",
    "from tqdm import tqdm\n",
    "thres = [0.07, 0.17, 0.2, 0.04, 0.23, 0.33, 0.24, 0.22, 0.1, 0.19, 0.23, 0.24, 0.12, 0.14, 0.25, 0.26, 0.16]\n",
    "preds = []\n",
    "for i in tqdm(range(result.shape[0]), miniters=1000):\n",
    "    a = result.ix[[i]]\n",
    "    a = a.apply(lambda x: x > 0.2, axis=1)\n",
    "    a = a.transpose()\n",
    "    a = a.loc[a[i] == True]\n",
    "    ' '.join(list(a.index))\n",
    "    preds.append(' '.join(list(a.index)))\n",
    "    \n",
    "df_test['tags'] = preds\n",
    "df_test.to_csv('submission_keras_5_fold_CV_0.9136_LB_0.913.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
